学习材料：[大模型推理框架RTP-LLM P-D分离之道：从思考到实战](https://mp.weixin.qq.com/s/4FVw5paNSUCeQEUp9hoJ5Q)

Chunked Prefill 技术将 Prefill 的请求拆成多个部分多轮执行，在每轮中和 Decode 请求凑批执行，可以提高 Decode 请求的交互性能，但是它的总时延还是会受到 Prefill 请求的影响。并且因为 Prefill 请求仍然长时间占用显存，导致 Decode 请求的并发受到限制。

P-D 分离可以带来一些好处：
- 它们可以选择不同的机型，Prefill 采用高算力的 GPU，Decode 采用大显存的 GPU。
- 它们可以选择**不同的量化方式和凑批大小**。这样就可以同时优化首字时间（TTFT）和Time Between Tokens（TBT）。
- 最重要的是不同请求的 Prefill 阶段的执行不再会影响 Decode 阶段，使得请求总时延和 P99 时延稳定。


学术界和工业界都有 P-D 分离技术的使用：
- Mooncake 是业界大规模使用 P-D 分离技术的例子。Mooncake 构建了以 KVCache 为中心的 P-D 分离调度集群
  - 形成了 Prefill/Decode Pool 以及分布式异构介质 KVCache Pool，
  - 全局负载均衡技术预测了 Prefill 和 Decode 的负载，在满足 SLA 的约束下最大化吞吐，
  - 在高负载下预测请求的执行时间，提供早停能力。
- Splitwise 开发了一个 GPU 集群
  - 该集群的设计将 LLM 推理过程分为两个独立的机器池，以优化不同阶段的处理效率。
  - 他们还增设了第三个机器池，专门用于处理 Prefill 和 Decode 阶段的混合批处理，
  - 能够根据实时计算需求灵活调整其规模。
- DistServe （阶跃星辰）通过将预填充和解码计算分配至不同的 GPU
  - DistServe 为每个阶段量身定制了资源分配和并行策略的优化方案。
  - DistServe 还考虑到服务集群的带宽，将这两个阶段放置在一起，以减少由任务分解所带来的通信开销。

## RTP-LLM 整体流程

![](https://mmbiz.qpic.cn/mmbiz_svg/1CHHx9Yq4nGyIxz5RczGpjszMeZhOzAj7nLOV3thibz8CNppBr2erp42nvh6xkmmKa8gf7vNFM4sz3lFsJ53H9GokF5vYibs1F/640?wx_fmt=svg&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

P-D分离的总体流程如上图所示：

- 请求到到达 Prefill 节点，检查是否可以开启 P-D 分离。
- Prefill 根据自己的负载均衡策略，选择 Decode 节点。
- Prefill 命令 Decode 申请显存资源。
- Prefill 将请求加入本地的任务队列，等待本地调度。
- Prefill 本地开始调度请求。
- Prefill 命令 Decode 拉取 KVCache。
- Prefill 生成 First Token，并且将 First Token 发送给 Decode。
- Decode 等待 KVCache 全部传输完毕，和 First Token 一起组织成任务，进行本地调度。
- Decode 流式产出 Token，返回给 Prefill，Prefill 流式返回给用户。
- Prefill 和 Decode 结束请求，清理资源。
- Prefill 的计算流和 RDMA 的发送之间通过 Cuda Event 来同步，从而实现了计算和传输的 Overlap。

kv cache 通过分层传输，RDMA 与计算流通过 cuda event 同步。

问题：
- [ ] prefill释放后如何做缓存
- [ ] prefill 和 decode 如何互相同步状态
- [ ] 机器挂掉怎么办

## KVCache传输， cache store

Mooncake [1] 采用了 Prefill 分层传输 KVCache 技术：在 Prefill 执行下一层计算的同时，将上一层的 KVCache 流式传输到 Decode。
在 Cache Store 中实现了两种传输方式：

- 在 TCP 实现中，Prefill 将显存中的 KVCache 拷贝一份到内存中，使用 [ARPC]() 通过 TCP 发送到对端。
- 在 RDMA 实现中，依赖通信库 **ACCL-Barex**，使用 GDR（GPUDirect RDMA [2]）在显存之间发送数据。

### TCP 传输

![tcp传输](https://mmbiz.qpic.cn/mmbiz_svg/1CHHx9Yq4nGyIxz5RczGpjszMeZhOzAjw5icibbVBvYrstzJIswwY52OWm7TCXfB6LV51fHc94dVlAHS0Cc4xO8HHXSnfib7hOw/640?wx_fmt=svg&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

总体流程基于 ARPC 实现（ARPC 是基于 ANet 实现的 RPC 框架，提供了面向 RDMA、TCP 等不同协议的 RPC 通信能力）

- Prefill 端按层计算产生 KVCache。
- 由显存拷贝到本地内存，放到 Cache Store中存储。
- Decode 端通过 ARPC 向 Prefill 发起 Load Cache 请求。
- Prefill 接收到请求后会在本地 CacheStore 中寻找请求对应的 Block，然后将找到的 Block 放入 ARPC的 Response 中，发回到 Decode。
- Decode 接收到 Response 后从中取出 Block，并拷贝回显存待使用。

协议过程有多次拷贝，分配内存和拷贝时延高且不稳定，并且还会影响计算流，多线程拷贝还带来了竞争问题。TCP 传输的问题较多，影响较大。

### RDMA传输

![rdma过程](https://mmbiz.qpic.cn/mmbiz_svg/1CHHx9Yq4nGyIxz5RczGpjszMeZhOzAjX62Qqyjg5rqRTbVuF6FWib1ya6zrB1TsVicDCZ7I5BGSicmueGulJwiaLicRqUuR0gCibw/640?wx_fmt=svg&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

DMA 下的 KVCache 传输流程如下，序号对应了上图过程：
- Prefill 端按层计算产生 KVCache。
- 由 Decode 向 Prefill **发起 ARPC 请求，告知对端自己的显存地址**。
- Prefill 接收到请求后调用 RDMA 的 Write 方法，将请求的 Block 从 Prefill 显存写入到 Decode 显存。
- Prefill 在写入完成后返回 ARPC Response。

ARPC tcp方式发显存地址，rdma直接写地址，ARPC传输控制信息。

基于 ACCL 通信库实现 RDMA 通信（ACCL 通信库是高性能网络团队研发的高性能通信库，支持基于服务器间的 RoCE、IB 和 TCP 网络实现高带宽和低延迟通信）。

### RDMA 通信模式
RDMA 的通信方式包括 Send、Recv、Read 和 Write 四种，
- Send、Recv 是一对类似 TCP 中的 read 和 write 的双边操作
- Read 和 Write 是 RDMA 特有的单边操作（单侧网卡发起，对端用户层不感知）。

在基于 RDMA 的 KVCache 传输中，数据传输可以由 Decode 发起，通过 Read 从 Prefill 传播到 Decode，也可以由 Prefill 发起，通过 Write 从 Prefill 传播到 Decode。

KVCache 的生命周期由 Prefill / Decode 各自独立管理。出于性能和安全性的考虑，使用 RDMA Write 模式来实现数据传输。整体流程
- 由 Decode 的 KVCache 分配器来分配显存，并告知 Prefill。
- Prefill 通过 RDMA 将自身的 KVCache Write 到对端。
- RDMA 传输所需的 remote_addr 和 rkey 等字段，我们通过 TCP 传输，RDMA 能力集中用于 KVCache 的发送。

### register memory region

- [ ] register mr
- [ ]  accl 队列
- [ ]  RDMA建连
  - [ ]  交换 QP 信息
  - [ ]  ModifyQP 
  - [ ]  RDMA 连接池
  - [ ]  多链接发送，并行化数据传输过程

在RDMA进行数据的发送和接收操作之前，必须先对源地址和目标地址执行Register MR操作。之所以要这么做，是因为存在一些网卡既没有配备IOMMU（输入/输出内存管理单元），又无法直接读取主机的MMU（内存管理单元）。在这种情况下，网卡需要通过Register MR操作来保存虚拟地址和物理地址之间的映射关系，以便后续能够正确地进行数据传输。

Register MR 比较耗时，所以我们在请求来临之前提前 Register MR。

程序启动时为 Prefill 和 Decode 申请所需显存并注册到 ACCL 中。Prefill 和 Decode 通过 cudaMemcpyAsync 申请出来的显存，无论如何都会 Register MR失败，Errno 为 EFAULT（Bad Address），令人百思不得其解。最后查清楚是 Nvidia 的 GDR 还不支持cudaMallocAsync，必须使用 cudaMalloc。

在实际使用中，MR 的注册总耗时极长。经过排查，确认原因是我们按照 Block 级别注册 MR，而按层分开的 Block 总数为几十万，导致注册时间达到了半个小时级别。我们优化了注册管理机制，采用注册大块显存的方式，将一整块显存，直接注册一次，注册时间降低到秒级。

Prefill 和 Decode 之间的 RDMA 传输使用多链接发送，并行化数据传输过程，减少单个连接的拥塞，从而提高整体传输速度。RDMA 支持多个发送和接收队列（Queue Pair，QP），可以为每个连接配置多个发送队列，这样可以在同一连接内部进一步实现并行发送，减少队列之间的等待时间，提升并发处理能力。在链接断开和 Decode 重启后能够自动重建 RDMA 连接，使得服务不会出现中断。

消息服务建立在通信双方本端和远端应用之间创建的Channel-IO连接之上。当应用需要通信时，就会创建一条Channel连接，每条Channel的首尾端点是两对Queue Pairs（QP）。

参考资料 [RDMA技术博客](https://blog.csdn.net/qq_21125183/category_7711389.html)

为了复用 System Prompt 和跨请求的KVCache：对于 System Prompt，我们将这块 KVCache 固定在 Block 内；对于跨请求的 KVCache，我们在请求执行完毕之时，将请求持有的 KVCache 资源释放给 LRU Cache，它将 KVCache 按照 Block Hash 标记；两种复用方式底层都使用 Trie 树组织。

初期我们的 KVCache Match 采用的是按照 Token ID 来寻找最长前缀匹配，这样在长序列以及 KVCache Item 比较多的时候，匹配效率较低。利用上述 Block Hash，我们可以按照 Block 级别快速寻找最长前缀匹配，大大降低了 BlockCache 的 Match 时延。

引擎在执行多个请求期间，可能总显存不足，此时我们会暂停某些请求的执行，释放部分 KVCache 资源，等待下次调度的时候，重新执行。这个策略在 Decode 引起了较大的问题，因为重新执行的请求会进入 Prefill 阶段，从而违背了 P-D 分离的初衷。所以在 P-D 分离的场景下，Prefill-Decode 禁止 KVCache 资源回退。

在线上部署的时候我们选择的是 16 个Prefill 实例对应 2 个 Decode 实例，Decode 机器承担了 8 倍的请求。我们观察到 Decode 容易出现爆显存，主要是因为并发执行的请求比较多。我们将 KVCache 量化减少 KVCache 占用，并且将 Decode 的模型权重从 FP16 替换成 INT4，使得模型权重显存占用降低到 1/4。此举大大提高了 Decode 的并行处理能力。

## 未来展望

- [ ] Ring Attention
- [ ] cpp
- [ ] megascale infer

### 长序列
在长文本场景下问题严重：首先长文本在 Prefill 执行时间比较长，在 Decode 爆显存。我们有一些想法来解决这个问题。在 Prefill 使用 CPP（Chunked Prefill Pipeline）/ PP (Pipeline Parallelism) / Ring Attention，将长文本的计算划分到多个 Prefill 节点执行。Decode 使用更大显存容量的卡，这样可以满足下长文本的 KVCache 占用。

解决 Decode 显存容量还有一个方法，那就是 Decode 选择更大的 TP Size，那么就和 Prefill 的 TP Size 不对等了。此时我们必须将 Prefill 的一个节点的 KVCache 进行划分，组织成 Decode 的 KVCache Layout。这必然带来一些代价，而且更大的 TP 意味着在计算过程出现更多的通信代价。但是更大的 TP 使得 Decode 的显存容量更大，可以并行执行更大的 Batch。


### ReuseCache
目前我们在 Prefill/Decode 各自使用了 Reuse Cache 逻辑，减少 KVCache 的计算和传输。进一步的，我们将 Decode 产出的 KVCache 也回传给 Prefill，有机会提高 Prefill 在下一轮会话的 KVCache 命中率。在 Prefill 自身压力比较大时，可以将请求转发给其他 Prefill 来降低压力，当然也可以从其他 Prefill 拉取 KVCache 来进行复用。


### 分布式 CacheStore

我们正在实现 Prefill-Decode、Prefill-Prefill、Decode-Decode 之间拉取 KVCache，使得 CacheStore 真正成为分布式 CacheStore。

### 负载均衡
服务的接入层根据 Prefill 的负载来选择 Prefill 节点发送请求，Prefill 再根据自己获取到所有 Decode 节点的剩余显存信息选择 Decode 节点来服务请求。

